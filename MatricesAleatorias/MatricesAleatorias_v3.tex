\chapter{Matrices aleatorias con estructura Hankel}\label{chap:RandomHankel}

%To set the problem, we quote the repeatedly cited phrase from Stewart and Sun, "our hero is the intrepid, yet sensitive matrix A. Our villain is E, who keeps perturbing A. When A is perturbed he puts on crumpled hat: $\tilde{A} = A + E$''.
	Considerando la aleatoriedad de la señal de perturbación $w_k$, es posible obtener una cota para el rango de $\Hank_{\x}$ cuando solo $\Hank_{\y}$ está disponible. Esto es de gran importancia en situaciones en las que la señal se encuentra perturbada por ruido y solo tenemos acceso a una parte de los datos. Una forma de abordar este problema es utilizar la teoría de matrices aleatorias para establecer umbrales basados en la distribución de los valores singulares. Esta metodología ofrece una herramienta poderosa para cuantificar la incertidumbre en la estimación del rango.
    
    Históricamente, este problema se ha estudiado principalmente en el contexto de matrices aleatorias sin estructura específica. Sin embargo, en este capítulo, se explora una perspectiva más específica al considerar la estructura Hankel de los datos. Se abordarán tanto los resultados relevantes previamente obtenidos en el contexto de matrices aleatorias sin estructura como las consideraciones específicas de la estructura Hankel, lo que permitirá encontrar un umbral para los valores singulares de ruido.

	El capítulo comienza con la sección \ref{sec:RTM} introduciendo algunos resultados conocidos sobre matrices aleatorias. Luego, en la sección \ref{sec:EDF} se discutirán las funciones de distribución empírica de los valores singulares para matrices aleatoria con entradas independientes e idénticamente distribuidas (iid) y matrices con estructura Hankel. Finalmente, en la sección \ref{sec:SpectNorm} se obtienen cotas probabilísticas para la norma espectral de matrices de Hankels.   

\section{Resultados preliminares sobre matrices aleatorias}\label{sec:RTM}

Una matriz aleatoria es simplemente una matriz en la que todas sus entradas son variables aleatorias, es decir, $\matA$ es una matriz de $m\times m$ tal que $\big[\matA\big]_{ij}$ son variables aleatorias en algún espacio de probabilidad $(\Omega,\mathcal{A},\P)$. Para cualquier instancia fija $\omega\in\Omega$, entonces $\matA(\omega)$ es una matriz de $m\times m$ con autovalores $\lambda_j(\omega)$, $j=1,2,\ldots,m$. Por lo tanto, los autovalores también son variables aleatorias.
Un resultado muy importante en la teoría de matrices aleatorias es encontrar la distribución de los autovalores o de los valores singulares de una matriz. Para ello, sea $\{\matX_n\}_{n\ge 0}$ una sucesión de matrices aleatorias $m\times n$ donde cada $\big[\matX\big]_{ij}$ son independientes e idénticamente distribuidas, con $\E\big[\big[\matX\big]_{ij}\big]=0$, $\E\big[\big[\matX\big]_{ij}^2\big]=1$ y $m=m(n)$. Se define la matriz
\begin{equation}
    S_n = \frac{1}{n}\matX_n\matX_n^T,
    \label{Eq:covMP}
\end{equation}
donde $\lambda_{1,n}\le\lambda_{2,n}\le\cdots\le\lambda_{m,n}$ son sus autovalores. 
Uno de los problemas principales en la teoría de matrices aleatorias es investigar la convergencia de la secuencia de distribuciones empíricas espectrales definidas como 
\begin{equation}
    F^{\matS_n}(x) = \frac{1}{m}\#\big\{ j\leq m:\lambda_{j,n}\leq x\big\} = \frac{1}{m}\sum_{i=1}^m\delta_{\lambda_{i,n}}
    \label{Eq:Spectralmeasure}
\end{equation}
La idea es encontrar una distribución límite $F(x)$, tal que,
\begin{equation}
    F^{\matS_n}(x) \xrightarrow[n\to \infty]{} F(x)
    \label{Eq:Convergence}
\end{equation}
utilizando algún criterio de convergencia. 

\begin{comment} 
\begin{theorem}\label{Th:MP1}
    Sea $\matS_n$ y $F^{\matS_n}(x)$ definidas en \eqref{Eq:covMP} y $\eqref{Eq:Spectralmeasure}$ respectivamente. Se asume $\frac{m}{n}\xrightarrow[n\to\infty]{} c\in(0,1]$. Luego, se tiene que 
    \begin{equation}
        F^{\matS_n}(x)\xrightarrow[n\to\infty]{a.s.}F_{MP}(x)
        \label{Eq:MP_law}
    \end{equation}
    donde $F_{MP}(x)$ es una medida de probabilidad cuya densidad viene dada por
    \begin{equation}
        f_{MP}(x) = \frac{1}{2\pi x c}\sqrt{(b-x)(x-a)}\one\{a\le x \le b\}
        \label{Eq:MP_law2}
    \end{equation}
    donde 
    \[a = (1-\sqrt{c})^2, \quad b = (1-\sqrt{c})^2.\]
\end{theorem}

La convergencia casi segura (a.s.) se cumple para cada elemento del espacio muestral donde la distribución espectral empírica converge puntualmente a la distribución $F_{MP}(x)$ salvo en un conjunto de probabilidad cero.% La convergencia casi segura, es decir salvo en un conjunto de probabilidad cero, se cumple que para cada $\omega$ del espacio muestral la distribución espectral empírica converge puntialmente a la distribución $F_{MP}(x)$.
En este caso se asumió que $c\le 1$. Para el caso $c>1$, dado que el rango de la matriz $\matS_n$ es $\min\{m,n\}$, entonces habrá $m-n$ ceros que contribuirán a tener una masa de probabilidad $(1-c^{-1})$ en $x=0$ en la distribución límite.
El comportamiento límite de la distribución empírica de la matriz fue estudiado por Marchenko y Pastur en \cite{Marchenko1967}. Desde entonces, varios artículos han perfeccionado estos resultados \cite{Bai2010}. Estos artículos proceden mediante un argumento combinatorio que involucra los momentos de los elementos de la matriz o emplean una herramienta llamada transformada de Stieltjes.
\end{comment}

\begin{theorem}\label{Th:MP1}
	Sea $\matS_n$ y $F^{\matS_n}(x)$ definidas en \eqref{Eq:covMP} y $\eqref{Eq:Spectralmeasure}$ respectivamente. Se asume $\frac{m}{n}\xrightarrow[n\to\infty]{} c\in(0,1]$. Luego, se tiene que con probabilidad 1
	\begin{equation}
		\lim\limits_{n\to\infty}F^{\matS_n}(x)=F_{MP}(x)
		\label{Eq:MP_law}
	\end{equation}
	donde $F_{MP}(x)$ es una medida de probabilidad cuya densidad viene dada por
	\begin{equation}
		f_{MP}(x) = \frac{1}{2\pi x c}\sqrt{(b-x)(x-a)}\one\{a\le x \le b\}
		\label{Eq:MP_law2}
	\end{equation}
	donde 
	\[a = (1-\sqrt{c})^2, \quad b = (1-\sqrt{c})^2.\]
\end{theorem}
La convergencia con probabilidad 1 o casi segura se cumple para cada elemento del espacio muestral donde la distribución espectral empírica converge puntualmente a la distribución $F_{MP}(x)$ salvo en un conjunto de probabilidad cero.

En este caso se asumió que $c\le 1$. Para el caso $c>1$, dado que el rango de la matriz $\matS_n$ es $\min\{m,n\}$, entonces habrá $m-n$ ceros que contribuirán a tener una masa de probabilidad $(1-c^{-1})$ en $x=0$ en la distribución límite.

Notar que, para una función continua y acotada $g:\R\to\R$, se tiene que
\[\int g(x)\mathrm{d}F^{\matS_n}(x) = \frac{1}{m}\sum_{i=1}^mg(\lambda_{i,n}),\]
que por la ley fuerte de los grandes números se obtiene la convergencia casi segura \cite{dudley2002}.
\begin{prop}\label{prop:SLLN}
    Sean $\matS_n$ definida en el teorema \eqref{Th:MP1}, y sus autovalores  $\{\lambda_{i,n}\}_{i=1}^m$. Sea una función $g:\R\to\R$ continua y acotada. Luego,
    \begin{equation}
        \frac{1}{m}\sum_{i=1}^m g(\lambda_{i,n})\xrightarrow[]{a.s} \int g(x)\mathrm{d}F_{MP}(x)
        \label{Eq:SLLN}
    \end{equation}
\end{prop}


El comportamiento límite de la distribución empírica de la matriz fue estudiado por Marchenko y Pastur en \cite{Marchenko1967}. Desde entonces, varios artículos han perfeccionado estos resultados \cite{Bai2010}. Estos artículos proceden mediante un argumento combinatorio que involucra los momentos de los elementos de la matriz o emplean una herramienta llamada transformada de Stieltjes.


\begin{comment}
A continuación se dará un esquema para la demostración del teorema \eqref{Th:MP1}

\begin{proof}
    Dado que el soporte de $f_{MP}(x)$ es compacto, $f_{MP}(x)$ está únicamente determinado por sus momentos. Por lo tanto, es suficiente demostrar que 
    \[\int x^k \mathrm{d}F^{\matS_n}(x)\to\int x^k\mathrm{d}F_{MP}(x).\]
    Gracias al lema de Borell-Cantelli basta mostrar lo siguiente
    \begin{equation}
        \E\bigg[\int x^k\mathrm{d}F^{\matS_n}(x)\bigg]\to \int x^k\mathrm{d}F_{MP}(x)
        \label{eq:proof_MP1}
    \end{equation}
    \begin{equation}
        \mathbb{V}\bigg[\int x^k\mathrm{d}F^{\matS_n}(x)\bigg] \le \frac{C_k}{n^2}
        \label{eq:proof_MP2}
    \end{equation}

    La integral a la derecha de \eqref{eq:proof_MP1} se puede escribir como
    \[\E\bigg[\int x^k\mathrm{d}F^{\matS_n}(x)\bigg] = \frac{1}{m}\E[\mathrm{tr}(\matS_n^k)] = \frac{1}{m}\E\bigg[\sum_{i=1}^m\lambda_i^k\bigg].\]
    Para resolver esta integral, es necesario usar teoría de combinatoria. Se puede probar que estas integrales están dada por lo números de Catalan, se deja al lector la demostración de (ver \cite{Bai2010}) 
    \[\E\bigg[\int x^k\mathrm{d}F^{\matS_n}(x)\bigg] \xrightarrow[n\to\infty]{} \sum_{r=0}^{k-1}\frac{c^r}{r+1}\binom{k}{r}\binom{k-1}{r}.\]
    Con el mismo argumento se puede probar \eqref{eq:proof_MP2}.
    
    El cálculo de la integral de la derecha en \eqref{eq:proof_MP1} se puede calcular fácilmente. Dado que $a+b = 2(1-c)$ y $ab = (1-c)^2$, se tiene
    \[\begin{aligned} \int x^k\mathrm{d}F_{MP}(x)  & = \int x^kf_{MP}(x)\mathrm{d}x = \frac{1}{2\pi c}\int_a^b x^{k-1} \sqrt{4c-(x-(1-c))^2}\mathrm{d}x\\[0.3em]  & = \frac{1}{2\pi}\int_{-2}^{2} (\sqrt{c}y+1+c)^{k-1} \sqrt{4-y^2}\mathrm{d}y, \quad y = (x-(1+c))/\sqrt{c} \\[0.3em] & = \frac{1}{2\pi}\sum_{r=0}^{k-1}c^{r/2}(1+c)^{k-1-r}\binom{k-1}{r}\int_{-2}^2y^r\sqrt{4-y^2} \quad \text{Expansión binomial}
    \\[0.3em] & = \sum_{r=0}^{(k-1)/2}c^r(1+c)^{k-1-2r}\binom{k-1}{2r}\binom{2r}{r}\frac{1}{1+r} \quad (*)\\[0.3em] & =\sum_{r=0}^{k-1}\frac{c^r}{r+1}\binom{k}{r}\binom{k-1}{r}\mathrm{d}y.\end{aligned}\]

    Donde en $(*)$ se obtiene a partir de los números de Catalan
    \[\frac{1}{2\pi}\int_{-2}^2y^{2r+1}\sqrt{4-y^2}\mathrm{d}y = 0, \quad \frac{1}{2\pi}\int_{-2}^2y^{2r}\sqrt{4-y^2}\mathrm{d}y = \frac{1}{1+r}\binom{2r}{r}. \]
    %https://arxiv.org/pdf/2203.02551.pdf
\end{proof}
\end{comment}

\section{Distribución empírica de los valores singulares}\label{sec:EDF}
	Una solución al problema \eqref{Prob:1} para la estimación del orden del modelo es inferir el $\rank(\Hank_{\x})$ contando los valores singulares relevantes de $\Hank_{\y}$. Dado un umbral $\tau$, el orden del modelo es estimado como
	\begin{equation}
		\hat{r}_{\mathrm{thr}} = \big|\{\sigma_i:\sigma_i>\tau\}\big|.
		\label{Eq:rankThreshold}
	\end{equation}
	Donde $|\cdot|$ es el cardinal del conjunto. La elección de $\tau$ es un problema delicado. Un $\tau$ muy grande  puede resultar en un rango más chico que el deseado. Por otro parte, si se tiene un $\tau$ muy chico puede llevar a sobrestimar el orden del modelo. Este problema fue abordado en \cite{Gavish2014}, donde se considera una matriz no aleatoria $\matX\in\C^{m\times n}$ tal que $\rank(\matX)=r\le\min\{m,n\}$. Sin embargo, solo se conoce una versión ruidosa $\matY\in\C^{m\times n}$ que obedece
	\begin{equation}
		\matY = \matX + \matW,
		\label{Eq:NoisyMatrix}
	\end{equation}
	donde $\matW$ es una matriz de ruido con entradas $[\matW]_{ij}$ independientes idénticamente distribuidos, media cero y varianza unitaria. Se quiere encontrar un estimador de $\matX$ que minimice el error cuadrático medio. La técnica por defecto se conoce como truncamiento de valores singulares (TSVD, \emph{Truncated Singular Value Decomposition}). Sea
	\begin{equation}
		\matY = \sum_{i=1}^{p}\sigma_i\u_i\v_i^H
		\label{Eq:SVDNoisyMatrix}
	\end{equation}
	la descomposición en valores singulares de la matriz $\matY$ con $p=\min\{m,n\}$. El estimador TSVD es
	\begin{equation}
		\hat{\matX} = \sum_{i=1}^r \sigma_i\u_i\v_i^H.
		\label{Eq:TSVD}
	\end{equation}
		
	El estimador en \eqref{Eq:TSVD} es la mejor aproximación de rango $r$ en el sentido de mínimos cuadrados \cite{Eckart1936}. Cuando el rango verdadero $r$ es desconocido, se trata de encontrar un estimado $\hat{r}$ para luego aplicar TSVD. Luego, se puede definir el siguiente estimador
	\begin{equation}
		\hat{\matX}_{\tau}= \sum_{i=1}^{p}y(\sigma_i,\tau)\u_i\v_i^H
		\label{Eq:HardThreshold}
	\end{equation}
	donde $y(\sigma,\tau) = \sigma\one\{\sigma\ge\tau\}$, en otras palabras, fuerza a cero cualquier valor singular que esté por debajo del umbral $\tau$. Este estimador se conoce como $SVHT$ (por sus siglas en inglés, \emph{Singular Values Hard-Threshold}). Este problema se puede escribir en forma equivalente como
	\begin{equation}
		\hat{\matX}_{\tau} = \arg\min_{\matB\in\C^{m\times n}} \big[\|\matY - \matB\|_F^2 + \tau^2\rank(\matB)\big].
		\label{Eq:PenalizedProblem}
	\end{equation}
		
	En la práctica, TSVD requiere estimar la varianza de ruido, así como la selección del parámetro $\tau$. Existen numerosos métodos en la literatura para elegir el umbral $\tau$. Los métodos heurísticos a menudo hacen uso de un \emph{scree plot} de los valores singulares, donde $\tau$ es elegido como la coordenada-y correspondiente al ``codo'' en la curva resultante.
		
	En \cite{Gavish2014} los autores se preguntan si hay una mejor elección para $\tau$ que funcione tan bien como TSVD en el sentido de mínimos cuadrados considerando ruido blanco. En este trabajo se considera un análisis asintótico en el siguiente sentido. Para $n\ge1$ se define $m_n = m(n)$ y las  matrices $\matY_n$, $\matX_n$, $\matW_n$ de $m_n\times n$, tal que  
	\begin{equation}
		\matY_n = \matX_n +\frac{\eta}{\sqrt{n}}\matW_n,
		\label{eq:AsymptoticModel}
	\end{equation}
	que satisfacen las siguientes propiedades
	\begin{itemize}
		\item Los elementos de $\matW_n$ son variables aleatorias independientes idénticamente distribuidas con media cero y varianza unitaria.
		\item Para todo $n$, $\rank(\matX_n) = r$, y su descomposición de valores singulares es 
		\[\matX_n = \matU_n\Lambdab\matV_n^H = \matU_n\diag(\lambda_1,\ldots,\lambda_r,0,\ldots,0)\matV_n^H,\]
		donde definimos $\veclambda = [\lambda_1, \lambda_2,\ldots,\lambda_r]\in\R^{r}$ con $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_r.$
		\item La secuencia $m_n$ 
		\[
			\frac{m_n}{n}\to c > 0,\quad n\to\infty.
		\]
	\end{itemize}
		
	El siguiente resultado se refiere al comportamiento de los valores singulares de la matriz $\matY_n$ cuando la matriz $\matX_n = \mathbf{0}$ \cite{Bai2010}.
		
	\begin{prop}\label{Prop:MP}
		Considerando $\matX_n =\mathbf{0}$, la distribución empírica de los valores singulares de $\matY_n$, $\sigma_1\ge\cdots,\ge\sigma_n$, converge en distribución una variable aleatoria con densidad
		\begin{equation}
			f_{MP}(x) = \frac{\sqrt{4c-(x^2-1-c)^2}}{\pi c\eta^2 x}\cdot\one\big\{x\in[1-\eta\sqrt{c},1+\eta\sqrt{c}]\big\}.
			\label{Eq:MPDensity}
		\end{equation}
		Además,
		\begin{equation}
			\sigma_n \to 1-\eta\sqrt{c}, \ \text{y } \ 
			\sigma_1 \to 1+\eta\sqrt{c}, 
		\end{equation}
		casi seguramente, cuando $n\to \infty$.
	\end{prop} 

    \begin{figure}[t]
		\begin{subfigure}[b]{0.5\linewidth}
			\centering
			\resizebox{0.9\linewidth}{!}{\input{Figuras/singular_values_iid_entries.pgf}}
			\caption{Matriz aleatoria con entradas iid.}
			\label{Fig:Histograma_SingularValues_iid}
		\end{subfigure}
		~
		\begin{subfigure}[b]{0.5\linewidth} 
			\centering
			\resizebox{0.9\linewidth}{!}{\input{Figuras/singular_values_Hankel_entries.pgf}}
			\caption{Matriz aleatoria con estructura Hankel}
			\label{Fig:Histograma_SingularValues_Hankel}
		\end{subfigure}
		\caption{Histograma de la ubicación de los valores singulares (azul), densidad de Marchenko-Pastur (rojo).}
		\label{Fig:svd1}
	\end{figure}
    En la Figura \ref{Fig:Histograma_SingularValues_iid} se muestra el histograma de la ubicación de los valores singulares de $\matY_m = \matW_n/\sqrt{n}$ para $m=n=512$ y la densidad de Marchenko-Pastur. Para este caso, sólo los valores singulares que son mayores que $1+\eta\sqrt{c}$ serán discernibles sobre el nivel de ruido.
		
	Sea $\hat{\matX}_{\tau,n}$ en \eqref{Eq:HardThreshold} la solución SVHT al problema de estimación. Se define el el error cuadrático medio asintótico (AMSE) como
	\begin{equation}
		\matM(\hat{\matX}_{\tau}, \veclambda) = \lim\limits_{m\to\infty}\big\|\hat{\matX}_{\tau,n} - \matX_n \big\|_F^2.
		\label{Eq:AMSE}
	\end{equation}
				
	Al adoptar un marco asintótico en \cite{Gavish2014}, los autores buscan el umbral óptimo para valores singulares que minimicen \eqref{Eq:AMSE}.
			
	\begin{theorem}\cite[Teorema 4]{Gavish2014}\label{Theo:Gavish}
		En el marco asintótico, para el AMSE del estimador SVHT en \eqref{Eq:HardThreshold} se tiene que  umbral óptimo es
		\begin{equation}
	   	    \tau_* = \kappa(c)\sqrt{n}\eta
		      \label{Eq:ThresholdGavish}
		\end{equation}
				
		\begin{equation}
			\kappa(c) = %\argmin_{\tau>1+\sqrt{c}}\max_{\veclambda\in\R^r} \matM(\hat{\matX}_{\tau},\veclambda) =
			\sqrt{2(c+1)+\frac{8c}{(c+1)+\sqrt{c^2+14c+1}}}. 
		\end{equation}
	\end{theorem} 
    \begin{proof}
        Sean $\big\{\matX_n\big\}_{n>0}$ y $\big\{\matW_n\big\}_{n>0}$ definidas en el enfoque asintótico \eqref{eq:AsymptoticModel}. Para el siguiente modelo
        \[\matY_n = \matX_n + \frac{\matW_n}{\sqrt{n}}.\]
        el SVTH se puede definir como
        \begin{equation}
            \hat{\matX}_{\kappa,n} = \sum_{i=1}^{m_n}y(\sigma_{n,i},\kappa)\u_{n,i}\v^H_{n,i}.
            \label{Eq:SVHT_natural}
        \end{equation}
        Si se trabaja con un modelo más general, $\matY_n = \matX_n + \eta\matW_n$, el SVHT se define como $\hat{\matX}_{\tau,n}$ donde la relación entre $\tau$ y $\kappa$ está dado por \begin{equation}\tau = \kappa\sqrt{n}\eta.\label{eq:tau_kappa}\end{equation}

        Siguiendo \cite{Gavish2014}, el AMSE de \eqref{Eq:SVHT_natural} se puede escribir como
        \begin{equation}
		      \matM(\hat{\matX}_{\kappa}, \veclambda) = \sum_{i=1}^r\matM(\hat{\matX}_{\kappa}, \lambda_i)
            \label{Eq:sumAMSE}
        \end{equation} 
        donde por \cite{Benaych2012} se obtiene que 
        \begin{equation}
	           \matM(\hat{\matX}_{\kappa}, \lambda) = \begin{cases} (\lambda + 1/\lambda)(\lambda + c/\lambda)-(\lambda^2-2c/\lambda^2) & \lambda\ge\lambda_*\\[0.3em]
		        \lambda^2 & \lambda <\lambda_*
	       \end{cases}
        \end{equation}
        con
        \[\lambda_* = \sqrt{\frac{\kappa^2-c-1+\sqrt{(\kappa^2-c-1)^2-4c}}{2}}.\]
        También se obtiene
        \begin{equation}
	\lim_{n\to\infty}y(\sigma_{n,i},\kappa)=\begin{cases} (\lambda_i+1/\lambda_i)(\lambda_i+c/\lambda_i) & (\lambda_i+1/\lambda_i)(\lambda_i+c/\lambda_i)\ge \kappa^2\\[0.3em]
		0 & (\lambda_i+1/\lambda_i)(\lambda_i+c/\lambda_i)< \kappa^2
	\end{cases}
\end{equation}
Considerando que $c\in(0,1]$ y que $\kappa>1+\sqrt{c}$. Sea $\lambda_*(\kappa)$ la única solución positiva de la ecuación
\[(\lambda + 1/\lambda)(\lambda + c/\lambda) = \kappa^2\]

Sea $\kappa_*$ la única solución a la ecuación en $\kappa$
\[\lambda_*^{4}(\kappa) - (c+1)\lambda_*^{2}(\kappa)-3c = 0.\]
Luego,

\[\arg\min_{\kappa >1+\sqrt{c}}\max_{\lambda>0}\matM(\hat{\matX}_{\kappa},\lambda) = \kappa(c)\]
Se obtiene que el umbral óptimo para el modelo  \eqref{Eq:SVHT_natural} es $\kappa(c)$. Finalmente, utilizando la relación \eqref{eq:tau_kappa} se obtiene \eqref{Eq:ThresholdGavish} para un modelo más general.
\end{proof}
			
	Este resultado indica que $\hat{\matX}_{\tau_*}$ siempre tiene un AMSE igual o mejor en comparación con cualquier estimador SVHT con $\tau\neq\tau_*$, y también en comparación con el estimador TSVD. En otras palabras, en \cite{Gavish2014} se considera que el estimador TSVD es asintóticamente inadmisible, así como también SVHT para cualquier umbral que no sea $\tau_*$, en especial para $\tau\le 1+\eta\sqrt{c}$
		
	Sin embargo, cuando se considera una suma de exponenciales, la matriz de perturbación hereda la estructura Hankel y sus entradas no son i.i.d. En este caso, la distribución empírica espectral converge a una medida de probabilidad no aleatoria que no tiene expresión explícita \cite{Bryc2006}. La figura \ref{Fig:Histograma_SingularValues_Hankel} muestra el histograma de los valores singulares de una matriz Hankel con $m=n=512$ contruida a partir del vector $\w\sim\Normal(\mathbf{0},\matI_{2n-1})$. En este caso, hay valores singulares que quedan fuera del soporte de la densidad de Marchenko-Pastur, por lo tanto la proposición \eqref{Prop:MP} no es válida, por lo que elegir el umbral siguiendo \eqref{Eq:ThresholdGavish} puede conducir a un rendimiento deficiente. 
		
	 
	\section{Norma espectral matrices  Hankel}\label{sec:SpectNorm}
	
		Sean las matrices $\Hank_{\x},\ \Hank_{\y}\in\C^{m\times n}$ definidas en el capitulo \eqref{chap:ModeloSumExp} con $m\ge n$ y sea  $\lambda_i$ y $\sigma_i$, $i = 1,\ldots,n$ sus valores singulares ordenados en forma decreciente. Debido al Teorema de Weyl se tiene la siguiente desigualdad
		\begin{equation}
			|\sigma_i-\lambda_i|\le \|\Hank_{\y} - \Hank_{\x}\|_2 = \|\Hank_{\w}\|_2.
			\label{Eq:WeylInequality}
		\end{equation}
		Debido a que $\rank(\Hank_{\x})=r$, $\lambda_i=0$ si $i>r$. Luego,
		\begin{equation}
			\sigma_n\le\sigma_{n-1}\le \cdots \le \|\Hank_{\w}\|_2.
			\label{Eq:WeylInequality2}
		\end{equation}
		
	\begin{lemma}\label{Lemma:2}
		Sea $N = m+n-1$, luego
		\begin{itemize}
			\item[i)] Si $\w\sim\mathcal{CN}(\mathbf{0},\eta^2\matI_{N})$, entonces 
			\begin{equation}
				\P[\|\Hank_{\w}\|_2 \leq \tau]\geq\bigg[1-e^{-\frac{\tau^2}{N\eta^2}}\bigg]^{N} \cdot\one\big\{\tau\geq 0\big\}.
				\label{Eq:BoundCirculantMatrix2}
			\end{equation}		 
			\item[] 
			\item[ii)] Si $\w\sim\mathcal{N}(\mathbf{0},\eta^2\matI_N)$, entonces, para $\tau\ge 0$
			\begin{equation}
				\P[\|\Hank_{\w}\|_2\leq \tau]  \geq \begin{cases} \mathrm{erf}^2\bigg(\frac{\tau}{\eta\sqrt{2N}}\bigg)\bigg[1-e^{-\frac{\tau^2}{N\eta^2}}\bigg]^{\frac{N}{2}-1} & \text{N par}\\[0.9em]
					\mathrm{erf}\bigg(\frac{\tau}{\eta\sqrt{2N}}\bigg)\bigg[1-e^{-\frac{\tau^2}{N\eta^2}}\bigg]^{\frac{N-1}{2}} & \text{N impar}
				 \end{cases}
			 \label{Eq:BoundCirculantMatrix2.1}
			\end{equation} 	
			donde $\mathrm{erf}(\cdot)$ es la función error, $\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}}\int_{0}^{x}e^{-t^2}\mathrm{d}t$
		\end{itemize}
	\end{lemma}
	\begin{proof}  Sea $\matC_{\w}\in\C^{N\times N}$ una matriz circulante asociada al vector $\w$. Gracias a la estructura de la matriz circulante, la matriz de Hankel $\Hank_{\w}\in\C^{m\times n}$ se puede escribir como
		\[\Hank_{\w} = \begin{bmatrix}\matT_m & \mathbf{0}_{m\times(n-1)}\end{bmatrix}\matC_{\w}\begin{bmatrix}\mathbf{0}_{(m-1)\times n}\\[0.3em] \matI_n
		\end{bmatrix},\]
		donde $\matT_m$ es la matriz identidad hacia atrás de $m\times m$. Es fácil ver que
		
		\[\|\Hank_{\w}\|_2\le \bigg\|\begin{bmatrix}\matT_m & \mathbf{0}_{m\times(n-1)}\end{bmatrix}\bigg\|_2\|\matC_{\w}\|_2\bigg\|\begin{bmatrix}\mathbf{0}_{(m-1)\times n}\\[0.3em] \matI_n
		\end{bmatrix}\bigg\|_2 \le \|\matC_{\w}\|_2,\]
		donde 
		\[\big\|\begin{bmatrix}\matT_m & \mathbf{0}_{m\times(n-1)}\end{bmatrix}\big\|_2\le 1,\quad \bigg\|\begin{bmatrix}\mathbf{0}_{(m-1)\times n}\\[0.3em] \matI_n
		\end{bmatrix}\bigg\|_2\le 1.\]
		Debido a que $\matC_{\w}$ es una matriz circulante, sus autovalores son $\e^T_{k+1}\matV\w$, donde $\matV$ es la matriz de transformada discreta de Fourier (DFT), y $\e_k$ es el $k-$ésimo vector unitario. Luego,
		\[\|\Hank_{\w}\|_2\le\|\matC_{\w}\|_2 = \max_k \big|\e^T_{k+1}\matV\w\big|\]
		
		Luego, según la distribución del vector $\w$ se obtendrán distintas distribuciones para $\max_k \big|\e^T_{k+1}\matV\w\big|$,
		\begin{itemize}
			\item[i)]
			Dado que $\w$ en vector aleatorio complejo con componentes iid Gaussianos y $\matV\matV^H = \matV^H\matV = N\matI_{N}$, el vector $\matV\w$ también es Gaussiano con componentes iid. Luego, para cualquier $\tau\ge0$
			
			\[\P\big[\max_k\big|\e_{k+1}^T\matV\w\big|\le \tau\big] = \prod_{i=1}^{N}\P\big[\big|\big[\matV\w\big]_i\big|\le \tau\big] = \bigg[1-e^{-\frac{\tau^2}{N\eta^2}}\bigg]^{N}.\]
			\item[] 
			\item[ii)] Cuando  $\w$ es un vector real, la DFT es par, es decir $\big[\matV\w\big]_i = \big[\matV\w\big]^*_{-i\mod N}$ $\forall i\in\{0,\ldots,N-1\}$. Además, para $N$ par $\big[\matV\w\big]_0$ y $\big[\matV\w\big]_{N/2}$ son reales, y la DFT estará completamente definida con los $N/2-1$ términos restantes. Por otro lado, cuando $N$ es impar, solo $\big[\matV\w\big]_0$ es un valor real y la DFT estará totalmente definida por los $(N-1)/2$ términos restantes. Luego
			\[\big[\matV\w\big]_i\sim \begin{cases}\Normal(0,N\eta^2) & \text{si $i = 0$ o $i=N/2$}\\[0.9em]  \NormalC(0,N\eta^2) & \text{otro caso.}\end{cases}\]
			Se define $Z_i = \big|\big[\matV\w\big]_i\big|$, que para todo $z\ge0$ tiene ls siguiente función de distribución
			\[\P[Z_i\le z] = \begin{cases}\mathrm{erf}\bigg(\frac{z}{\eta\sqrt{2N}}\bigg) & \text{si $i = 0$ o $i=N/2$}\\[0.9em] \big[1-e^{-\frac{z^2}{\eta^2N}}\big] & \text{otro caso.} \end{cases}\]
			Finalmente, para un $\tau\ge0$ se obtiene
			\[\P\big[\max_k\big|\e_{k+1}^T\matV\w\big|\le \tau\big] = \begin{cases} \mathrm{erf}^2\bigg(\frac{\tau}{\eta\sqrt{2N}}\bigg)\bigg[1-e^{-\frac{\tau^2}{N\eta^2}}\bigg]^{\frac{N}{2}-1} & \text{N par}\\[0.9em]
				\mathrm{erf}\bigg(\frac{\tau}{\eta\sqrt{2N}}\bigg)\bigg[1-e^{-\frac{\tau^2}{N\eta^2}}\bigg]^{\frac{N-1}{2}} & \text{N impar}
			\end{cases}\]
		\end{itemize}\end{proof}
	
	Cuando se tiene un vector normal complejo, gracias a la expresión \eqref{Eq:BoundCirculantMatrix2} del Lema \eqref{Lemma:2} es posible encontrar un cota probabilística para la norma espectral de una matriz de Hankel aleatoria.
	
	\begin{theorem}\label{Th:boundHankelMatrix2}
		Sea $\Hank_{\w}$ una matriz aleatoria Hankel de $m\times n$ construida a partir del vector $\w \sim\mathcal{CN}(\mathbf{0},\eta^2\matI_{m+n-1})$. Luego, para cualquier $\beta\in [0,1]$, se obtiene que
		\[ \P[\|\Hank_{\w}\|_2\le \tau_1]\ge\beta.
		\]
		donde 
		\begin{equation}
			\begin{aligned} 
				\tau_1 & =  \eta\sqrt{-(m+n-1)\log(1-\beta^{\frac{1}{m+n-1}})}. 
			\end{aligned}
			\label{Eq:BoundHankelMatrix2}
		\end{equation}
	\end{theorem}				
	\begin{proof}
		El resultado de este teorema sigue inmediatamente del Lema~\ref{Lemma:2} y la expresión \eqref{Eq:BoundCirculantMatrix2} eligiendo $\beta$ de modo que
		\[\beta \leq \bigg[1-e^{-\frac{\tau_1^2}{(m+n-1)\eta^2}}\bigg]^{m+n-1}. \]
	\end{proof}
	
	Desafortunadamente, no es posible encontrar una expresión para la cota superior de la norma espectral de la matriz de Hankel aleatoria con entradas reales debido a la expresión de la función de distribución \eqref{Eq:BoundCirculantMatrix2.1} en el Lema \eqref{Lemma:2}. Sin embrago, se puede obtener la siguiente desigualdad de concentración 
	
	
	\begin{lemma}\label{Th:boundHankelMatrix}
		Sea $\w\sim\Normal(\mathbf{0},\matI_{N})$, con $N = m+n-1$. Luego, para  $\Hank_{\w}\in\R^{m\times n}$ se obtiene que 
		\begin{equation}
			\P[\|\Hank_{\w}\|_2\geq t] \leq (m+n) \exp\bigg[-\frac{t^2}{2\eta^2\max(m,n)}\bigg]
			\label{Eq:BoundHankelMatrix}
		\end{equation}
	
	Además, para cualquier $\beta\in[0,1]$, se obtiene que
	\[\P\big[\|\Hank_{\w}\|_2\le\tau\big]\ge \beta,\]
	donde 
	\begin{equation}
		\tau = \eta\sqrt{-2\max\{m,n\}\log\bigg(\frac{1-\beta}{m+n}\bigg)}
		\label{Eq:BoundHankelMatrix1}
	\end{equation}
	\end{lemma}				
	\begin{proof}
		See Appendix \ref{Proof:BoundHankelMatrix}.
	\end{proof}

	La expresión \eqref{Eq:BoundHankelMatrix} del Lema \eqref{Th:boundHankelMatrix} también se obtuvo en \cite{Qiao2020} para el caso de matrices de Toeplitz cuadradas. 
	
	Para analizar la cota obtenida en \eqref{Eq:BoundHankelMatrix2}, se realizarán simulaciones de Monte Carlo calculando la norma espectral de la matriz de Hankel $\Hank_\w\in\C^{n\times n}$ asociada a la realización $\w\sim\NormalC(\mathbf{0},\eta^2\matI_{2n-1})$. Para destacar la dispersión de los valores singulares de la matriz de Hankel, también se calcula la cota obtenida en el teorema \ref{Theo:Gavish} para matrices cuadradas con entradas iid
	\begin{equation}
		\tau_2 = \frac{4}{\sqrt{3}}\sqrt{n}\eta.
		\label{eq:gavish}
	\end{equation}
	La Figura~\ref{fig:taus11} muestra el comportamiento de $\|\Hank_\w\|_2$ en función de la varianza de las entradas de la matriz, $\eta^2$. Cada Figura corresponde a un valor diferente de $n$. El área sombreada representa la dispersión entre las realizaciones de $\|\Hank_\w\|_2$ junto a las cotas $\tau_1$ y $\tau_2$. A medida que la dimensión de las matrices aumenta, $\tau_2$ se acerca al valor medio de $\|\Hank_\w\|_2$. Consecuentemente, cuando se estima el rango de la matriz como el número de valores singulares que están por encima de $\tau_2$, se pueden tener en cuenta valores singulares asociados al espacio de ruido,  como fue observado en  la Fig.~\ref{Fig:Histograma_SingularValues_Hankel}. Por otro lado, en la Fig.~\ref{fig:taus11} se observa que $\tau_1$ es una cota conservativa, por lo que es posible que algunos valores singulares correspondientes al espacio de señal se encuentren por debajo de este umbral. Para superar este problema, en la siguiente sección, se formulará una solución al problema de estimar el rango de la matriz de Hankel ruidosa mediante un problema de optimización con restricciones.
	
	\begin{figure}[t]
		\centering
		\begin{subfigure}{0.4\linewidth}
			\centering
			\includegraphics[width = \linewidth]{Figuras/taus_N_64_beta_0.9.pdf}
			\caption{$n = 64$}
		\end{subfigure}
		\begin{subfigure}{0.4\linewidth}
			\centering
			\includegraphics[width = \linewidth]{Figuras/taus_N_128_beta_0.9.pdf}
			\caption{$n = 128$}
		\end{subfigure}
		\begin{subfigure}{0.4\linewidth}
			\centering
			\includegraphics[width = \linewidth]{Figuras/taus_N_256_beta_0.9.pdf}
			\caption{$n = 256$}
		\end{subfigure}
		\begin{subfigure}{0.4\linewidth}
			\centering
			\includegraphics[width = \linewidth]{Figuras/taus_N_512_beta_0.9.pdf}
			\caption{$n = 512$}
		\end{subfigure}
		\caption{Realizaciones de $\|\Hank_\w\|_2$ y cotas $\tau_1$ y $\tau_2$ para diferentes dimensiones $n$ con $\beta = 0.9$}
		\label{fig:taus11}
	\end{figure}

\newpage

\section{Apéndice}
	\subsection{Demostración del Lema \ref{Th:boundHankelMatrix}}\label{Proof:BoundHankelMatrix}

	Para demostrar el Lema \ref{Th:boundHankelMatrix} se utilizará el siguiente resultado
	\begin{lemma}\cite[Chap. 4]{tropp2015}\label{lemma:RandomMatrixIneq}
	Considera la secuencia finita $\{\matA_k\}$ de matrices de $m\times n$ fijas y sea $\{\gamma_k\}$ una secuencias finita de variables aleatorias independientes normales estándar. Se define la siguiente serie
	\begin{equation}
		\matY = \sum_k\gamma_k\matA_k.
		\label{Eq:GaussSeries}
	\end{equation}
	Se define $\nu(\matY)$  como 
	\begin{equation}
		\begin{aligned} 
			\nu(\matY) & = \max\{\|\E[\matY\matY^H]\|_2,\|\E[\matY^H\matY]\|_2\}\\[0.3em]
			& = \max\{\|\sum_k\mathbf{A}_k\mathbf{A}_k^H\|_2,\|\sum_k\mathbf{A}_k^H\mathbf{A}_k\|_2\},
		\end{aligned}
	\end{equation}
	Luego, para todo $t\geq 0$,
	\begin{equation}
		\P[\|\matY\|_2\geq t]\leq (m+n)\exp\bigg[-\frac{t^2}{2\nu(\matY)}\bigg]
		\label{Eq:boundGaussSeries}
	\end{equation}						
\end{lemma}

A continuación se demuestra el Lema \ref{Th:boundHankelMatrix}. Considere el vector $\w\sim\Normal(\mathbf{0},\eta\matI_{N})$, donde $N = m+n-1$ y se asume que $m>n$ (la demostración para $m<n$ es equivalente). La matriz de Hankel asociada al vector $\w$ se puede escribir como																
\begin{equation}
	\Hank_{\w} = w_{m-1}\matJ + \sum_{k=1}^{m-1}
	w_{m-k-1}\matB^k\matJ + \sum_{k=1}^{n-1}w_{m+k-1}(\matB^k)^H\matJ,
	\label{Eq:HankExpression}
\end{equation}
donde 
\[\matB = \begin{bmatrix} 0 & 1 & 		  &        &  \\
	& 0 & 1      &        &  \\
	&   & \ddots & \ddots &   \\
	&   &        &  0     & 1 \\
	&   &        &        & 0
\end{bmatrix}\in\R^{m\times m}, \quad 
\matJ = \begin{bmatrix} 0      & 0 & \cdots & 0 \\
						\vdots     & \vdots & \cdots & \vdots \\
						0      & 0 & \cdots & 1 \\
						\vdots &   & \reflectbox{$\ddots$} & \vdots \\
						0      & 1 &        & 0\\
						1      & 0 &        & 0
\end{bmatrix}\in\R^{m\times n}.\]

Resulta que $\matJ^H\matJ = \matI_n$ y que 
\[\matJ^H(\matB^k)^H\matB^k\matJ = \sum_{i=1}^{\min(m-k,n)}\matE_{ii},\quad \matJ^H\matB^k(\matB^k)^H\matJ = \sum_{i=k+1}^{n}\matE_{ii},\]
donde $\matE_{ii}$ es una matriz con entradas iguales a  $1$ en la posición $(i,i)$ (diagonal principal) y ceros en las otras posiciones.

De la expresión \eqref{Eq:HankExpression}, se obtiene que
\[\begin{aligned}
	\matI_n + \sum_{k=1}^{m-1}\matJ^H(\matB^k)^H\matB^k\matJ + \sum_{k=1}^{n-1}\matJ^H\matB^k(\matB^k)^H\matJ & = \matI_n + \sum_{k=1}^{m-1}\sum_{i=1}^{\min(m-k,n)}\matE_{ii} + \sum_{k=1}^{n-1}\sum_{i=k+1}^{n}\matE_{ii}\bigg]\\[0.3em]
	& = \sum_{i=1}^n\bigg[1 + \sum_{k=1}^{m-i}1+\sum_{k=1}^{i-1}1\bigg]\matE_{ii} \\
	& = \sum_{i=1}^n(1+(m-i)+(i-1))\matE_{ii} = m\matI_n.
\end{aligned}\]	
Luego, tomando la esperanza se obtiene que 
\[\big\|\E[\Hank_{\w}^H\Hank_{\w}]\big\|_2 = \|\eta^2m\matI_n\|_2 = \eta^2m.\]

Operando de la misma forma se puede obtener que 
\[\|\E[\Hank_{\w}\Hank_{\w}^H]\|_2 = \|\eta^2n\matI_m\|_2 = \eta^2n.\]

Por lo tanto, 
\[\nu(\Hank_{\w}) = \eta^2\max\{m,n\} = \eta^2m.\]

Por el Lema~\ref{lemma:RandomMatrixIneq}, se obtiene la cota \eqref{Eq:BoundHankelMatrix}. 

Por otra parte, la expresión \eqref{Eq:BoundHankelMatrix1} se obtiene eligiendo $\beta$ de modo que
\[1-\beta \ge (m+n)e^{-\frac{t^2}{2\eta^2\max\{m,n\}}}. \]






%\section*{Apéndice}
%\addcontentsline{toc}{section}{\bfseries Apéndices}
%\renewcommand{\thesubsection}{\arabic{subsection}}
%\setcounter{subsection}{0}

