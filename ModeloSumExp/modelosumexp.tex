\chapter{Modelo de suma de exponenciales}\label{chap:ModeloSumExp}

	Un problema presente en muchas aplicaciones de procesamiento de señales es recuperar  información útil  a partir de datos en el dominio del tiempo. La señal a detectar suele contener parámetros desconocidos tales como amplitud, fase, frecuencia, etc. 

	Los métodos de subespacios \cite{Stoica2005} son herramientas adecuadas para la estimación espectral. Los enfoques tradicionales incluyen la estimación de parámetros de señal mediante la técnica de invariancia rotacional (ESPRIT) \cite{Roy1989} o el método del \emph{matrix pencil} (MPM) \cite{Hua1990}, donde las frecuencias estimadas se obtienen mediante la solución de un problema de autovalores generalizados. 


	\section{Descripción del modelo}\label{sec:ModelDescription}

		El modelo,
		\begin{equation}
			y_k = x_k + w_k,\qquad k=0,1,\ldots,
			\label{Eq:noisySignal2}
		\end{equation}
		donde $x_k$ viene dado por
		\begin{equation}
			x_k = \sum_{i=1}^{r}c_iz_i^k  \qquad k = 0,1,\ldots, 
			\label{Eq:Eq:NoiselessSignal}
		\end{equation}
		donde $z_i\in\C$ es una exponencial compleja, y $c_i\in\C$ es la amplitud asociada a $z_i$ y $r$ es el orden del modelo. En \eqref{Eq:noisySignal2}, $w_k$ es un proceso Gaussiano complejo circularmente simétrico, $w_k\sim\NormalC(0,\eta^2)$, 
		
		Se introducen las siguientes definiciones que resultaran útiles a lo largo de este capítulo.
	
	
		\begin{definition}[Matriz Hankel]\label{Def:Hankel}
			Una matriz con estructura Hankel es una matriz en la que cada diagonal ascendente desde la izquierda a la derecha es contante. En términos de los elementos de la matriz, se tiene que $[\matX]_{ij} = [\matX]_{i+j,j+k}$ si $k = 0,\ldots,j-i$, con $i\le j$. Se define el espacio de las matrices Hankel como
			\begin{equation}
				\Hank = \big\{\matX\in\C^{m\times n}:\matX \text{ tiene estructura Hankel}\big\}.
				\label{Eq:HankelSet}
			\end{equation}
		\end{definition}
	
		A partir de un vector de dimensión $m+n-1$, $\x = [x_0,x_1,\ldots,x_{m+n-2}]^T$ se denota $\Hank_{\x}$ la matriz con estructura Hankel tal que $[\Hank_{\x}]_{ij} = x_{i+j-2}$. A partir de esta definición, la norma de Frobenius de una matriz de Hankel se puede escribir como
		\begin{equation}
			\|\Hank_{\x}\|_F^2 = \sum_{k=0}^{m+n-2}\alpha_k|x_k|^2
			\label{Eq:Frobnorm}
		\end{equation}
		donde
		\[\alpha_k = \begin{cases} k+1 &  0 \leq k < m\\
		m &  m \leq k \leq n\\
		m+n-k & n < k \leq n+m-1.
		\end{cases}\]
	
		\begin{definition}
			Se define el conjunto de matrices con rango igual o menor que $r\in\N$ como
			\begin{equation}
				\setC_{(r)} = \big\{\matX\in\C^{m\times n}:\rank(\matX)\leq r\big\}.
				\label{Eq:SetRank_r}
			\end{equation}
		\end{definition}

%	Se denota la función indicadora sobre un conjunto $\setX$ como
%	\begin{equation}
%		\chi_{\setX}(\x) = \begin{cases} 0 & \x\in\setX,\\
%			\infty & \x\notin\setX.
%		\end{cases} 
%		\label{eq:indicator}
%	\end{equation}
%	
	
	\section{Métodos de Subespacios}

	Dado un $m>r$, para la señal \eqref{Eq:Eq:NoiselessSignal}, usando la definición \eqref{Def:Hankel} se escribe la matriz de $(m+1)\times r$ con estructura Hankel asociada al vector $\x = [x_0,x_1, \ldots,x_{m+r-1}]^T$ como

	\begin{equation}
		\Hank_{\x} = \begin{bmatrix} x_0 & x_1 & \cdots & x_{r-1} \\[0.3em]											x_1 & x_2 & \cdots & x_{r} \\[0.3em]
		\vdots & \vdots & \ddots & \vdots \\[0.3em]
		x_{m} & x_{m+1} & \cdots & x_{m+r-1}\\[0.3em]
		\end{bmatrix}
		\label{Eq:HankMatrix_1}
	\end{equation}
	Esta matriz de Hankel admite un descomposición de Vandermonde,

	\begin{equation}
		\Hank_{\x} = \matZ_m\matC\matZ_{r-1}^T,
		\label{Hdecomposition:eq}
	\end{equation}
	donde  $\matC =\diag(c_1,\cdots,c_r)$, y $\matZ_m$ y $\matZ_{r-1}$ son matrices de Vandermonde 
	\begin{equation}
		\matZ_l = \begin{bmatrix} 1 & \cdots & 1 \\[0.3em] 
		z_1  & \cdots & z_r \\[0.3em] 
		\vdots  &        & \vdots \\[0.3em]
		z_1^{l} & \cdots & z_r^{l} \\[0.3em]
		\end{bmatrix},
		\label{Eq:VandermondeMatrix}
	\end{equation}

	El siguiente resultado establece una relación entre la combinación lineal de $r$ exponenciales complejas y una matriz de Hankel infinita con rango $r$.

	\begin{theorem}[Kronecker 		\cite{Gantmacher1960}]\label{Th:Kronecker}
		Sea el vector $\x = \{x_k\}_{k=0}^\infty$ definido a partir de la muestras de \eqref{Eq:Eq:NoiselessSignal}. Luego la matriz de Hankel infinita definida como 
		\begin{equation}
			\Hank_{\x} = \begin{bmatrix} x_0 & x_1 & x_2 & \cdots \\[0.3em] x_1 & x_2 & x_3 & \cdots \\[0.3em] x_2 & x_3 & x_4 & \cdots \\[0.3em]\vdots & \vdots & \vdots & \ddots \end{bmatrix}=\{x_{i+j}\}_{i,j=0}^{\infty}
			\label{Eq:InfiniteHankelMatrix}
		\end{equation}
		tiene rango finito igual a $r$ 
	\end{theorem}
	\begin{proof}
		Si $\x$ es definida por \eqref{Eq:Eq:NoiselessSignal}, se define el polinomio característico como
		\begin{equation}
			p(z) = \prod_{i=1}^{r}(z-z_i) = \sum_{k=0}^{r} q_kz^k.
			\label{Eq:CharacteristicPolynomial}
		\end{equation}	
		Luego,
		\begin{equation}
			\sum_{k=0}^{r}q_kx_{j+k} = \sum_{k=0}^rq_k\sum_{i=1}^{r}c_iz_i^{j+k} = \sum_{i=1}^{r}c_iz_i^j\bigg(\sum_{k=0}^{r}q_kz_i^k\bigg) = 0
		\end{equation}
		para todo $j\in\N$, es decir, la $(r+j)-$ésima columna de $\Hank_{\x}$ es combinación de las $r$ columnas anteriores. Por lo tanto, $\rank(\Hank_{\x})\le r$. 
	
		A continuación, se quiere probar que $\rank(\Hank_{\x}) = r$. Dada la estructura de \eqref{Eq:Eq:NoiselessSignal} usando la descomposición de Vandermonde para la matriz de Hankel truncada
		\[\begin{bmatrix} x_0 & x_1 & \cdots & x_{r-1}\\[0.3em] x_1 & x_2 & \cdots & x_r\\[0.3em] \vdots & \vdots & \ddots & \vdots\\[0.3em] x_{r-1} & x_r & \cdots & x_{2r-2}
		\end{bmatrix} = \matZ_{r-1}\matC\matZ_{r-1}^T,\]
		donde $\matZ_{r-1}$ es la matriz de Vandermonde definida en \eqref{Eq:VandermondeMatrix} y $\matC = \diag(c_1,\ldots,c_r)$. Como ambos $z_i$ y $c_i$ son diferentes para todo $i=1,\ldots,r$, las matrices $\matZ_{r-1}$ y $\matC$ tienen rango completo. Por lo tanto, las primeras $r$ columnas (filas) de $\Hank_{\x}$ son linealmente independientes y $\rank(\Hank_{\x}) = r$. 	
	
		Contrariamente, si la matriz de Hankel infinita tiene rango $r$, la  ($r+j$)-iésima columna de $\Hank_{\x}$ es una combinación lineal de las $r$ columnas previas. Luego, el vector $\x$ satisface la siguiente ecuación en diferencias de orden $r$					
		\begin{equation}
			x_{j+r}= -\sum_{k=0}^{r-1} q_kx_{j+k} \quad \forall j=0,1,2\ldots.
			\label{Eq:PronyEquation} 
		\end{equation}
		Una solución posible a esta ecuación es $x_j=0$ para todo $j\in\N_0$. Otra posible solución es  tomar $x_j=z^j$ para algún $z\in\C$, $z\neq 0$. Luego, asumiendo que $q_r=-1$, se obtiene el siguiente resultado
		\begin{equation}
			\sum_{k=0}^{r}q_kx_{j+k} = \sum_{k=0}^rq_kz^{j+k} = z^j\sum_{k=0}^{r}q_kz^k = z^jP_q(z),
			\label{Eq:PronyEquation1}
		\end{equation}
		siendo $P_q(z)$ es polinomio característico de la ecuación en diferencias \eqref{Eq:PronyEquation} con raices $z_i$ con $i=1,\ldots,r$. Por lo tanto, \eqref{Eq:PronyEquation1} es igual a cero si y solo si $P_q(z) = 0$. Finalmente, $x_j=z^j$ es una solución no trivial de la ecuación homogénea \eqref{Eq:PronyEquation} si y solo si $z$ es una raíz del polinomio característico $P_q(z)$ y $\x$ se puede escribir como $\eqref{Eq:Eq:NoiselessSignal}$.
	\end{proof}

	Definiendo el vector $\x_r=[x_r,\ldots,x_{m+r}]$, del Teorema de Kronecker, la matriz de Hankel satisface el siguiente sistema (ecuaciones \eqref{Eq:PronyEquation} y \eqref{Eq:PronyEquation1})
	\begin{equation}
		\Hank_{\x} \q = -\x_r,
		\label{prony:eq}
	\end{equation}
	donde $\q = [q_0 \cdots q_{r-1}]^T$ son los coeficientes del polinomio característico 
	\begin{equation} 
		q(z) = z^{r}+\sum_{k=0}^{r-1}q_kz^{k},
		\label{Eq:prony_pol}
	\end{equation}
	tal que $q(z_i)=0$ para $i=1, \ldots , r$.  Esta sencilla configuración es lo que se conoce como el método de Prony para estimar los modos $z_i$ \cite{Prony1975}. El algoritmo \eqref{Algorithm_prony} resume los pasos del método.
	\begin{algorithm}
		\caption{Método de Prony}
		\begin{algorithmic}[1]
			\State {Entradas: $\mathbf{H}\in\C^{(m+1)\times r}$ y $\mathbf{x}_r = [x_r \cdots x_{m+r}]^T$.}
			\State {Resolver \eqref{prony:eq} }
			\State {Determinar las raíces $z_i$, $i = 1,\ldots, r$ del polinomio de Prony \eqref{Eq:prony_pol}}	
		\end{algorithmic}
		\label{Algorithm_prony}
	\end{algorithm} 

	Desafortunadamente, $\Hank_{\x}$ es una matriz mal condicionada y resolver \eqref{prony:eq} para $\q$ puede ser muy sensible para datos ruidosos. Además, obtener los $z_i$ como las raíces de $q(z)$ también puede resultar una tarea difícil cuando \eqref{Eq:Eq:NoiselessSignal} se ve perturbada por el ruido.

	Por otro lado, usando la descomposición de Vandermonde \eqref{Hdecomposition:eq} se han  propuesto enfoques más robustos. Considere la descomposición en valores singulares de $\Hank_{\x} = \matP\matS\matQ^H$, donde las columnas de $\matP$ generan el espacio columna de $\Hank_{\x}$ y se indentifican con el subespacio de señal en $\C^{m+1}$. Como los $z_i$  son todos distintos y $m>r$, se obtiene que $\rank(\matZ_m)=\rank(\matZ_{r-1})=r$. Por lo tanto, la columnas de $\matZ_m$ también generan subespacio de señal. Entonces, existe una matriz invertible $\matG\in\C^{r\times r}$ tal que 
	\begin{equation}
		\matZ_m  = \matP \matG.
		\label{Zm:eq}
	\end{equation}

	Definiendo las siguientes matrices
	\begin{align}
		\matZ_{m,l} &= \left[\begin{array}{c}
		\matI_{m-1} \\ \mathbf{0}_{1\times m-1}
		\end{array}\right]^T\matZ_m \qquad \matZ_{m,f} = \left[\begin{array}{c}\mathbf{0}_{1\times m-1} \\ \matI_{m-1}\end{array}\right]^T\matZ_m.
		\label{vandermonde_lf:eq}\\
		\matP_{l} &=\left[\begin{array}{c}
		\matI_{m-1} \\ \mathbf{0}_{1\times m-1}
		\end{array}\right]^T\matP \qquad \matP_{f} = \left[\begin{array}{c}\mathbf{0}_{1\times m-1} \\ \matI_{m-1}\end{array}\right]^T\matP. \nonumber
	\end{align}
	donde las matrices $\matZ_{m,f}$ ($\matP_f$) y $\matZ_{m,l}$ ($\matP_l$) se obtiene a partir de $\matZ_{m}$ ($\matP$) borrando la primera y última fila respectivamente, es fácil verificar la propiedad de invarianza rotacional dada por
	\begin{equation}
		\matZ_{m,f} = \matZ_{m,l}\matZ,
		\label{rotationalprinciple:eq}
	\end{equation}
	donde $\mathbf{Z} = \diag(z_1, \ldots z_r)$. Reemplazando \eqref{Zm:eq} en  \eqref{vandermonde_lf:eq}, se obtiene que  $\matZ_{m,l}  = \matP_l \matG$ y 
	$\matZ_{m,f}  = \matP_f \matG$. Por lo tanto, de \eqref{rotationalprinciple:eq} se obtiene que 
	\begin{equation}
		\matP_{f} \matG = \matP_{l}\matG\matZ. 
		\label{Esprit:eq}
	\end{equation}
	Esta última ecuación conduce a un problema de autovalores generalizados para obtener las frecuencias desconocidas $z_1,\ldots, z_r$ y caracteriza el algoritmo \eqref{Algorithm_esprit} conocido como ESPRIT  \cite{Roy1989}.

	\begin{algorithm}
		\caption{ESPRIT}
		\begin{algorithmic}[1]
			\State {Entradas: $\Hank_{\x}\in\C^{(m+1)\times r}$.}
			\State {Hallar la descomposición en valores singulares de $\Hank_{\x} = \matP\matS\matQ^H$.}
			\State {Construir las matrices $\matP_f$, $\matP_l$.}
			\State {Hallar las raíces $z_i$, $i = 1,\ldots, r$ resolviendo  $\matP_{f} \v = z \matP_{l}\v$.}	
		\end{algorithmic}
		\label{Algorithm_esprit}
	\end{algorithm}	

	Un	método relacionado explota la relación entre $	\Hank_{\x,f}$ y $\Hank_{\x,l}$, definidas de acuerdo con \eqref{vandermonde_lf:eq}. Usando  \eqref{Hdecomposition:eq} y \eqref{rotationalprinciple:eq}, se obtienen
	\begin{equation}
		\Hank_{\x,l} = \matZ_{m,l}\matD\matZ_{r-1}^T, \qquad 	\Hank_{\x,f} = \matZ_{m,l}\matZ\matD\matZ_{n-1}^T.
		\label{eq:VandermondeDecomposition}
	\end{equation}

	Para algún $z\in \C$, 
	\[\Hank_{\x,f} -z \Hank_{\x,l} = \matZ_{m,l}(\matZ-z\matI_r)\matD\matZ_{r-1}^T.
	\]

	Dado que $\matZ_{m,l}$, $\matD$, y $\matZ_{r-1}$ son matrices de rango completo, los entradas en la diagonal de $\matZ$ son las soluciones del problema de autovalores generalizados
	\begin{equation}
		\Hank_{\x,f} \v = z \Hank_{\x,l}\v.
		\label{MPM:eq}
	\end{equation}
	Esta método se conoce como Matrix Pencil (MPM) \cite{Hua1990} debido a que el conjunto  $\{\Hank_{\x,f} -z \Hank_{\x,l}; z \in \C\}$ forma lo que se conoce como un \emph{matrix pencil}. Los pasos del método se muestran en el algoritmo \eqref{Algorithm_MPM}.

	\begin{algorithm}
		\caption{Matrix Pencil Method}
		\begin{algorithmic}[1]
			\State {Entrada: $\Hank_{\x}\in\C^{(m+1)\times r}$.}
			\State {Construir las matrices $\Hank_{\x,f}$, $\Hank_{\x,l}$.}
			\State {Determinar las raíces $z_i$, $i = 1,\ldots, r$ resolviendo $\Hank_{\x,f}\v = z \Hank_{\x,l}\v$.}	
		\end{algorithmic}
		\label{Algorithm_MPM}
	\end{algorithm}		
	Notar que en el caso sin ruido, \eqref{Esprit:eq} y \eqref{MPM:eq} son equivalentes, y tanto MPM como ESPRIT obtienen las mismas soluciones \cite{Hua1991}.	

	Como se mencionó anteriormente las primeras $r$ columnas de $\matP$ forman un base ortornormal que genera el subespacio columna de $\matZ_m$. Por lo tanto, las columnas de la matriz $\matP^\perp = [\p_{r+1}\ldots\p_{m}]$ generan un subespacio que es ortogonal al generado por $\{\p_1,\ldots,\p_r\}$. Luego, la proyección ortogonal de las columna de la matriz $\matZ_m$ sobre el subspacio generado por las columnas $\matP^\perp$ es 

	\begin{equation}
		\matP^\perp(\matP^\perp)^H\z_i = \mathbf{0}, \qquad i = 1,\ldots,r,
	\end{equation}
	donde $\z_i = [1\ z_i\ \cdots\  z_i^{m}]^T$.

	El método MUSIC (multiple signal classification) \cite{Schmidt1986} se basa en este principio. Por lo tanto, los $z_i$ son hallados determinando la ubicación de los $r$ mínimos de la función 
	\begin{equation} P_{MUSIC}(\z) = \|(\matP^{\perp})^H\z\|_2^2.\label{Eq:MUSIC_eq}\end{equation}
	La precisión de este método está limitada por la discretización en la que se evalúa la función \eqref{Eq:MUSIC_eq}. % Discretización de que.

	%In \cite{Stoica1991} is shown that ESPRIT outperforms MUSIC.				

	\begin{algorithm}
		\caption{MUSIC}
		\begin{algorithmic}[1]
			\State {Entradas: $\Hank_{\x}\in\C^{(m+1)\times r}$.}
			\State {Descomposición en valores singulares de $\Hank_{\x} = \matP\matS\matQ^H$.}
			\State {Construir la matriz $\matP^\perp$.}
			\State {Determinar las raíces $z_i$, $i = 1,\ldots, r$ resolviendo $P_{MUSIC}(\mathbf{z}) = \mathbf{0}$.}	
		\end{algorithmic}
		\label{Algorithm_music}
	\end{algorithm}	

	Cabe destacar que los métodos introducidos hasta aquí se basan en suposiciones donde el parámetro $r$ es conocido y la señal no está contaminada con ruido aditivo. 

	Generalmente, el parámetro $r$ es desconocido y debe ser estimado a partir de los datos antes del problema de estimación. Incluso si $r$ es conocido, la señal puede estar contaminada con ruido y debe ser preprocesada con el fin de mitigar el efecto del ruido. 

	En los capítulos subsiguientes se abordará el estudio de la mitigación del ruido y se mostrará que aunque uno obtenga una buena estimación de la señal el cálculo de autovalores mediante ESPRIT o MPM puede resultar inestable. %Por otro lado, también se abordará el problema de estimación del parámetro $r$. En ambos casos se propondrán soluciones novedosas para contrarrestar estos problemas.


	\section{Métodos de reducción de ruido}

	Generalmente, la señal puede estar contaminada con ruido aditivo y debe ser preprocesada con el fin de mitigar el efecto del ruido. Asumiendo el modelo de señal ruidosa definido en \eqref{Eq:noisySignal2}, se forma la matriz $\Hank_{\y}$ con estructura Hankel. Si $r$ es desconocido, el rango de $\Hank_{\y}$ es $n>r$.

	%Debido a que el número de frecuencias es conocido o fue estimado previamente, en un primer enfoque, se busca la estimación del espacio columna de $\Hank_{\y}$ a partir de su descomposición en valores singulares. Este problema puede ser formulado de la siguiente manera

	\begin{Prob}\label{Prob:1}
		Dada un matriz $\Hank_{\y}\in\C^{m\times n}$, $r\in\N$ tal que $1\le r\le \min\{m,n\}$ encontrar una solución $\matH^\star\in\C^{m\times n}$ a 
		\begin{equation}
			\begin{aligned} 
				& \min_{\matH\in\C^{m\times n}} \quad \|\Hank_{\y}-\matH\|_F^2\\[0.3em]
				& \text{sujeto a} \quad \rank(\matH) \le r.
			\end{aligned}
			\label{Eq:LowRank_Approximation1}
		\end{equation}
	\end{Prob}
	Este problema fue resuelto en \cite{Eckart1936} y la solución es 
	\begin{equation}
		\matH^\star = \svd_r(\Hank_{\y}) =\bigg\{ \sum_{i=1}^r\sigma_i\u_i\v_i^H : \sum_{i=1}^{\min\{m,n\}}\sigma_i\u_i\v_i^H  \text{ es la SVD de } \Hank_{\y}\bigg\}
		\label{Eq:LowRank_Approximation2}
	\end{equation}
	es decir se retienen los valores singulares dominantes de la matriz de Hankel ajustando a cero los restantes de modo de obtener una aproximación de bajo rango. Desafortunadamente, el problema \eqref{Prob:1} no es convexo debido a la restricción sobre el rango, y la solución \eqref{Eq:LowRank_Approximation2} puede no ser única. Además, la solución resultante en \eqref{Eq:LowRank_Approximation2} no preserva la estructura Hankel.  

	Usando el Teorema \ref{Th:Kronecker} se puede formular un problema de optimización con los requisitos de que la solución esté en la intersección entre el subespacio de matrices con rango $r$ y el subespacio que contiene matrices con estructura Hankel. Para un $r$ dado, se plantea el siguiente problema de optimización

	\begin{Prob}\label{Prob:minCadzow}
		Dada la matriz $\Hank_{\y}\in\C^{m\times n}$, $r\in\N$ tal que $1\le r\le \min\{m,n\}$ encontrar la solución $\matH^\star\in\C^{m\times n}$ a			
		\begin{equation}
			\begin{aligned}
				& \min_{\matH\in\Hank} \quad \|\Hank_{\y}-\matH\|_F \\[0.3em]
				& \text{sujeto a}\quad \rank(\matH)\le r
			\end{aligned}
			\label{Eq:minCadzow}
		\end{equation}
	\end{Prob}
	Debido a que la restricción sobre el rango es no convexa, el problema \eqref{Prob:minCadzow} es difícil de resolver. En \cite{Cadzow1988} se propone un procedimiento iterativo donde en cada iteración se alternan proyecciones en el espacio de matrices de rango $r$ con proyecciones en el espacio de matrices Hankel. Primero se obtiene una aproximación de bajo rango mediante \eqref{Eq:LowRank_Approximation2}, es decir se obtiene, 
	\begin{equation}
		\matX = \svd_r(\Hank_{\y}).
	\end{equation}
	Luego, se obtiene la aproximación de Hankel donde los elementos $(k,l)-$ésimos de $\matH^\star$ se obtiene promediando los elementos de las antidiagonales de $\matX$, es decir
	\begin{equation}
		[\matH^\star]_{kl} = \frac{1}{|\Omega_{k+l}|}\sum_{(k',l')\in\Omega_{k+l}}[\matX]_{k'l'},
		\label{Eq:HankelProjection}
	\end{equation}
	donde $\Omega_{k+l}$ es el conjunto de índices correspondiente al $(k+l)-$ésima antidiagonal de $\matX$, y $|\Omega_{k+l}|$ es el cardinal del conjunto.		


	Otro enfoque se obtiene al relajar la restricción sobre el rango por su envolvente compleja dada por la norma nuclear \cite{Fazel2001,Recht2010}.	
	\begin{equation}
		\min_{\matH\in\Hank} \|\Hank_{\y}-\matH\|_F + \mu\|\matH\|_*,
		\label{Eq:minNuclear}
	\end{equation}
	donde $\mu\in\R_{\geq 0}$. La idea es penalizar la suma de valores singulares para setear a cero varios de ellos. Sin embargo, la elección del valor de $\mu$ suele ser un desafío. Por un lado elegir un $\mu$ muy grande podría dar como resultado una matriz con un rango más pequeño que el deseado, mientras que un $\mu$ pequeño puede resultar en un rango grande.

	Teniendo en cuenta el teorema \eqref{Th:Kronecker}, en un trabajo mas reciente \cite{Andersson2014}, se propone el siguiente problema  
	\begin{equation}
		\begin{aligned} 
			& \min_{\hat{\y}} \quad \|\y - \hat{\y}\|_2^2 \\[0.3em]
			& \text{sujeto a} \quad  \rank(\Hank_{\hat{\y}}) = r.
		\end{aligned}
		\label{Eq:Andersson1}
	\end{equation}
	donde la matriz de Hankel $\Hank_{\hat{\y}}\in\C^{m\times n}$ se define a partir del vector $\hat{\y} = [\hat{y}_0, \hat{y}_1,\ldots, \hat{y}_{m+n-2}]^T$.

	Debido a la restricción en el rango de la matriz este en un problema no convexo. Sin embargo, se puede reformular como	
	\begin{equation}
		\begin{aligned} 
			& \min_{\matA,\hat{\y}} \quad \chi_{\setC_{(r)}}(\matA) + \|\y-\hat{\y}\|_2^2\\[0.3em]
			& \text{sujeto a} \quad  \matA = \Hank_{\hat{\y}},
		\end{aligned}
		\label{Eq:Adenrsson2}			 
	\end{equation}
	donde $\chi_{\setC_{(r)}}(\matA)$ es la función indicadora sobre el conjunto de matrices de rango $r$.

	Este problema se puede resolver mediante el método de multiplicadores alternados (ADMM)\cite{Boyd2011} (Ver Apéndice). A continuación se define la función langrangeana aumentada del problema \eqref{Eq:Adenrsson2} como
	\begin{equation}
		\L(\matA,\hat{\y},\Lambdab) =\chi_{\setC_{(r)}}(\matA) + \|\y-\hat{\y}\|_2^2  + \langle\Lambdab,\matA-\Hank_{\hat{\y}}\rangle + \frac{\rho}{2}\|\matA - \Hank_{\hat{\y}}\|_F^2,
		\label{Eq:Andersson3}
	\end{equation}
	donde $\Lambdab\in\R^{(m+1)\times n}$ es el matriz de multiplicadores de Lagrange y $\rho$ es un parámetro de penalidad constante. La función \eqref{Eq:Andersson3} es similar al lagrangeano convencional excepto por el término cuadrático. Éste se hace cero cuando $\matA = \Hank_{\hat{\y}}$, es decir, cuando la solución es factible. 

	El lagrangiano aumentado se puede reescribir de la siguiente manera
	\begin{equation}
		\L(\matA,\hat{\y},\Lambdab) = \chi_{\setC_{(r)}}(\matA) + \|\y-\hat{\y}\|_2^2 + \frac{\rho}{2}\|\matA-\Hank_{\hat{\y}}+\frac{1}{\rho}\Lambdab\|^2_F -\frac{1}{2}\|\Lambdab\|_F^2
		\label{Eq:Andersson4} 
	\end{equation}

	Para minimizar \eqref{Eq:Andersson4}, ADMM usa una estrategia iterativa, es decir dados $\matA^{i}, \hat{\y}^{i}$ y $\Lambdab^i$ en el paso $i-$ésimo, las variables $\matA$, $\y$ y $\Lambdab$ se actualizan de la siguiente forma 
	\begin{itemize}
		\item[a)] Se obtiene la matriz $\matA$ resolviendo el siguiente problema de optimización
			\begin{equation} 
				\matA^{i+1}  = \argmin_{\matA}\L(\matA,\hat{\y}^i,\Lambdab^i) = \arg\min_{\matA} \bigg\{\chi_{\setC_{(r)}}(\matA) + \frac{\rho}{2}\|\matA - \Hank_{\hat{\y}^i}+\frac{1}{\rho}\Lambdab^i\|_F^2\bigg\}
				\label{Eq:Andersson5}				
			\end{equation} 
			Este problema se resuelve como la proyección de $\Hank_{\hat{\y}^i}-\frac{1}{\rho}\Lambdab^i$ sobre el espacio $\setC_{(r)}$,
			\begin{equation}
				\matA^{i+1} = \svd_r(\Hank_{\hat{\y}^i}-\frac{1}{\rho}\Lambdab^{i})
			\end{equation}
	
		\item[b)] A continuación, para actualizar la variable $\y$ se resuelve el siguiente problema de optimización
			\begin{equation}
				\hat{\y}^{i+1} = \argmin_{\hat{\y}} \L(\matA^{i+1},\hat{\y},\Lambdab^i) = \arg\min_{\hat{\y}} \bigg\{\|\y - \hat{\y}\|_2^2 + \frac{\rho}{2}\|\matA^{i+1} - \Hank_{\hat{\y}}+\frac{1}{\rho}\Lambdab^i\|_F^2\bigg\}
				\label{Eq:Andersson6}
			\end{equation}
			Se introduce el siguiente vector
			\begin{equation}
				\b = \bigg[\sum_{(k',l')\in\Omega_{k+l}}\big[\matA^{i+1}+\frac{1}{\rho}\Lambdab^{i}\big]_{k'l'}\bigg]_{j=0}^{m+n-2}
			\end{equation}
			donde cada componente de $\b$ es la suma de las antidiagonales de $\matA^{i+1}+\Lambdab^{i}/\rho$. También, el término $\|\matA^{i+1}+\Lambdab^{i}/\rho - \Hank_{\hat{\y}}\|_F^2$, se puede aproximar mediante \eqref{Eq:Frobnorm} como
			\[\sum_{i=0}^{m+n-2}\alpha_i|\hat{y}_i-b_i/\alpha_i|^2.\]
			Reemplazando en \eqref{Eq:Andersson6} se obtiene
			\begin{equation}
				\hat{\y}^{i+1} = \argmin_{\hat{\y}}\sum_{i=0}^{m+n-2}|y_i-\hat{y}_i|^2 + \frac{\rho}{2}\sum_{i=0}^{m+n-2}\alpha_i|\hat{y}_i-b_i/\alpha_i|^2.
			\end{equation}
			Finalmente por cuadrados mínimos se obtiene
			\begin{equation}
				\hat{\y}^{i+1} = (\matI_{m+n-1}-\rho\diag(\vecalpha))^{-1}(\y + \rho\b),
			\end{equation}
			con $\vecalpha = [\alpha_1,\alpha_2,\ldots,\alpha_{m+n-2}]^T$ donde cada componente es la cantidad de elementos que hay en las antidigonales de la matriz de Hankel.
	
			\item[c)] Finalmente actualizamos $\Lambdab$ de la siguiente forma
				\begin{equation}
					\Lambdab^{i+1} = \Lambda^i + \rho(\matA^{i+1}-\Hank_{\hat{\y}^{i+1}}).
					\label{Eq:Andersson7}
				\end{equation}
		\end{itemize}

		Los pasos a seguir están resumidos en el Algoritmo \eqref{Algorithm_Andersson}. En \cite{Andersson2014} se toma $\rho = 0.025$ y se itera unas 200 veces. 

		\begin{algorithm}
			\caption{Pseudo-código para el algoritmo en \cite{Andersson2014}}
			\begin{algorithmic}[1]
				\State{\textbf{Entradas}: Señal ruidosa $\y\in\C^{m+n-1}$, rango de la matriz de Hankel $r$ y parámetro de regularización $\rho$.}
				\State{\textbf{Salidas}: reconstrucción de $\x$ a partir de $\y$.}
				\State{Inicializar $\Lambdab$, $i$}
				\While{$i<200$}
					\State{Actualizar $\matA^{i+1}$ resolviendo \eqref{Eq:Andersson5}.}
					\State{Actualizar $\y^{i+1}$ resolviendo \eqref{Eq:Andersson6}.}
					\State{Actualizar $\Lambdab^{i+1}$ resolviendo \eqref{Eq:Andersson7}.}
					\State{$i = i + 1$.}
				\EndWhile	
			\end{algorithmic}
			\label{Algorithm_Andersson}
		\end{algorithm} 

		El objetivo de este procedimiento, es siempre obtener una secuencia aproximada $\hat{\y}$ y su matriz de Hankel asociada $\matA = \Hank_{\hat{\y}}$. Luego, para estimar los $z_i$, $i=1,\ldots,r$ se pueden usar los Algoritmos \eqref{Algorithm_esprit} o \eqref{Algorithm_MPM} como un segundo paso.

		%Sin embargo, esta formulación sigue siendo un problema no-convexo. 
	\section{Estabilidad Numérica}
	
		La sensibilidad de la estimación de los autovalores con respecto a la perturbación de la perturbaciones en la señal se puede calcular a partir del problema del haz matricial. En \cite{Golub1999, Beckermann2007} se estudia este problema considerando la secuencia $\x = \big[x_0,\ x_1,\ \cdots,\ x_{2r-1} \big]$. A partir de esta secuencia, se plantea el problemas de autovalores generalizados $\Hank_{\x,f}\v = z\Hank_{\x,l}\v$, con $\Hank_{\x,f},\Hank_{\x,l}\in\C^{r\times r}$.
		
		Considere un autovalor simple del haz matricial, se puede escribe el problema de perturbación como 
		\begin{equation}
			(\Hank_{\x,f}+\epsilon\matF)(\v + \epsilon\dot{\v}+\cdots) = (z+\epsilon\dot{z}+\cdots)(\Hank_{\x,l}+\epsilon\matE)(\v + \epsilon\dot{\v}+\cdots),
			\label{Eq:eps_Perturbation}
		\end{equation}
		donde $\matF$ y $\matE$ son dos matrices de perturbación, $\epsilon\in\R$ y $\dot{\v}$ y $\dot{z}$ son las aproximaciones de primer orden. Por lo tanto, reteniendo solo los términos de primer orden, se obtiene
		\begin{equation}
				(\Hank_{\x,f}-z\Hank_{\x,l})\dot{\v} = (\dot{z}\Hank_{\x,l} + z\matE - \matF)\v, 
			\label{Eq:eps_Perturbation_1erOrden}
		\end{equation}
		se desea encontrar una expresión para $\cdot{z}$ que mida la sensibilidad de primer orden. A continuación se multiplica \eqref{Eq:eps_Perturbation_1erOrden} a la izquierda por el autovector (izquierdo) $\v$ (que en este caso coincide con el autovector derecho, ya que $\Hank_{\x,l}$ y $\Hank_{\x,f}$ son matrices cuadradas simétricas complejas). Esta acción anula el lado izquierdo de \eqref{Eq:eps_Perturbation_1erOrden}, y después de simplificar se obtiene
		\begin{equation}
			\dot{z} = \frac{\v^H(\matF - z\matG)\v}{|\v^H\Hank_{\x,l}\v|}
			\label{Eq:firstOrderPerturbation}
		\end{equation}
	
		En \cite{Golub1999} se asume que tanto $\matF$ como $\matE$ no tiene estructura Hankel y además $\|\matF\|_2 = \|\Hank_{\x,f}\|_2$ y $\|\matE\|_2 = \|\Hank_{\x,l}\|_2$, obteniéndose
		\begin{equation}
			|\dot{z}_i| \le \frac{\|\Hank_{\x,f}\|_2 + |z_i|\|\Hank_{\x,l}\|_2}{|\v^H\Hank_{\x,l}\v|}
			\label{Eq:firstOrderPerturbation2}
		\end{equation}
		El término importante en \eqref{Eq:firstOrderPerturbation2} es el denominador; cuando este es pequeño, se dice que el autovalor está mal condicionado. También lo sera, si la matrices de Hankel están mal condicionadas.
		
		Por otro lado, en \cite{Beckermann2007}, se obtiene otro cota superior restringiendo las matrices $\matF$ y $\matE$ a tener estructura Hankel y$\|\matF\|_2, \|\matE\|_2<1$  . De esta forma se obtiene,
		\begin{equation}
			|\dot{z}_i| \le \frac{(1 + |z_i|)\|\matZ_{r-1}^{-1}\e_i\|_2}{|c_i|}.
			\label{Eq:firstOrderPerturbation3}
		\end{equation}
		Para este caso, el autovalor estará mal condicionado cuando $|c_i|$ es pequeño, o cuando la matriz de Vandermonde está mal condicionada.
		
		En la capítulo \ref{chap:EstabilidadNumerica} se extenderán estos resultados para problemas de autovalores generalizados con matrices Hankel rectangulares asociadas a secuencias $\x = \big[x_0,\ x_1,\ \cdots,\ x_{N-1} \big]$ para $N>2r$. Se demostrará que la aproximación de primer orden para el autovalor será inversamente proporcional a la energía asociado a ese autovalor, pero también será inversamente proporcional a la distancia con la que se encuentran los autovalores entre sí.

	
	
	
	
	
		
	\newpage
	\section{Apéndice}
	%\addcontentsline{toc}{section}{\bfseries Apéndices}
	%\renewcommand{\thesubsection}{\arabic{subsection}}
	%\setcounter{subsection}{0}
	
	\subsection{Método de multiplicadores de Lagrange con direcciones alternadas (ADMM)}
	
	El método de multiplicadores con direcciones alternadas o ADMM (\emph{Alternating Direction Method od Multipliers}) \cite{Boyd2011} es un algoritmo desarrollado para problemas en optimización convexa. Toma un enfoque de \emph{dividir y conquistar} donde las soluciones a subproblemas locales se coordinan para encontrar un solución al problema global. ADMM presenta las ventajas de los métodos de descomposición dual y del Lagrangiano aumentado para problemas de optimización con restricciones \cite{Parikh2014}. En particular, se usa para resolver problemas tales como
	\begin{equation}
		\min_{\x} f(\x) + g(\x),
		\label{Eq:ADMM_apx1}
	\end{equation}
	que es equivalente al siguiente problema con restricciones.
	\begin{equation}
		\begin{aligned} 
			& \min_{\x,\z}\quad f(\x) + g(\z), \\[0.3em]
			& \text{sujeto a}\quad \x = \z
		\end{aligned}
		\label{eq:dr1}
	\end{equation}
	con $f(\cdot)$ y $g(\cdot)$ funciones convexas.
	
	El ADMM se puede ver como una versión mejorada del método de multiplicadores en el caso donde la función objetivo es separable. El Lagrangiano aumentado asociado con \eqref{eq:dr1} es la función 
	\begin{equation}
		\L(\x,\y,\veclambda) = f(\x) + g(\z) +\langle\veclambda,\x-\y\rangle + \frac{\rho}{2}\|\x-\z\|^2_2,
		\label{eq:dr2}
	\end{equation}
	donde $\rho\ge 0$ es un parámetro de penalidad y $\veclambda$ es el multiplicador de Lagrange. Usando el método de multiplicadores se resuelve el problema \eqref{eq:dr1} minimizando iterativamente $\L(\x,\y,\veclambda)$ sobre las variables $\x$ e $\y$ mientras se actualiza el multiplicador de Lagrange $\veclambda$ (conocido como variable dual). Sin embargo, esto requiere minimizar el Lagrangiano conjuntamente en $\x$ e $\y$. Para evitar esta situación, el ADMM aprovecha la separabilidad de la función objetivo y divide el procedimiento de minimización en dos pasos separados, uno para cada variable.  Específicamente, reescribiendo el Lagrangiano como
	\begin{equation}
		\L(\x,\y,\veclambda) = f(\x) + g(\z) +\frac{1}{2}\|\x-\z+\veclambda\|_2^2 - \frac{1}{2}\|\veclambda\|_2^2
		\label{eq:dr4}
	\end{equation}
	y tomando un $\rho \ge 0$, el paso iterativo para resolver ADMM para resolver \eqref{eq:dr1} es
	\begin{equation}
		\begin{aligned}
			& \x^{k+1} = \arg\min_{\x}\L(\x,\veclambda^k,\z^k) = \arg\min_{\x} f(\x) + \frac{1}{2}\|\x - \z^k + \veclambda^k\|_2^2 \\[0.3em]
			& \z^{k+1} = \arg\min_{\z}\L(\x^{k+1},\veclambda^k,\z) = \arg\min_{\z} g(\z) + \frac{1}{2}\|\z - \x^{k+1}-\veclambda^k\|_2^2 \\[0.3em]
			& \veclambda^{k+1} = \arg\min_{x}\L(\x^{k+1},\veclambda,\z^{k+1}) = \veclambda^k + \x^{k+1}-\z^{k+1}. \\[0.3em]
		\end{aligned}
		\label{eq:dr5}
	\end{equation}
	
	Aunque esto no es equivalente a la minimización exacta, la convergencia de este esquema está bien establecida en el caso de que $f(\cdot)$ y $g(\cdot$) sean convexas \cite{Parikh2014}.
	
	
	
	
